<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[7.编译Spark源码]]></title>
    <url>%2F2018%2F12%2F13%2F7.%E7%BC%96%E8%AF%91Spark%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[如果我们要使用spark，最好是根据自己的环境把源码编译一次。而且我们已经有了编译hadoop的经验，这个做起来比编译hadoop简单哈。 环境准备硬件环境：2核2线程 8G内存 40G硬盘 Linux环境：CentOS 6.5 jdk版本：jdk1.8 scala版本：2.12.8 maven版本：apache-maven-3.6.0 Hadoop版本：hadoop-2.6.0-cdh5.7.0 git版本：1.7.1 下载spark源码前往github上面的spark源码地址https://github.com/apache/spark/ 这个是目前最新的代码，我们要选择一个版本，点击branch，选择master最近的一个版本 如上图所示，我们选择的版本就是2.4了，点击Clone or download按钮，我们下载源码包spark-branch-2.4.zip 上传spark源码将我们得到的zip上传到我们虚拟机上面 123# su - hadoop$ cd ~/software$ rz 解压spark源码1$ unzip spark-branch-2.4.zip -D ../source 修改pom文件12$ cd ~/source/spark-branch-2.4$ vi pom.xml 1.修改maven仓库地址，为了提速我们把repositories替换成如下配置(原来的可以删除掉) 1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;!-- &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; --&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;Cloudera Repository&lt;/name&gt; &lt;!-- &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; --&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 2.插件地址也替换掉，找到pluginRepositories 123456&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 修改make-distribution.sh为了提速需要将判断版本的语句注释，我们直接写入版本信息 1$ vi ~/source/spark-branch-2.4/dev/make-distribution.sh 123456789101112131415161718192021222324VERSION=2.4.0SCALA_VERSION=2.12SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0SPARK_HIVE=1#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use "set -o pipefail"# echo -n) 至此我们完成所有配置修改 编译spark123456789$ cd ~/source/spark-branch-2.4$ ./dev/make-distribution.sh \--name 2.6.0-cdh5.7.0 \--tgz \-Phadoop-2.6 \-Phive \-Phive-thriftserver \-Pyarn \-Dhadoop.version=2.6.0-cdh5.7.0 输入完这个命令，就开始编译了。一开始有个语句，我很不安 编译的时候，我会在这个步骤卡得比较久 cpu飚的得很高 错误1 好的，没错。失败了。我果然很有先见之明 我手工去下载zinc-0.3.15.tgz包,错误提示的地址果然下载不了，我把地址里面https改成http又愉快的下载起来了 12$ cd /home/hadoop/source/spark-branch-2.4/build$ rz 然后重新跑起来 123456789$ cd /home/hadoop/source/spark-branch-2.4$ ./dev/make-distribution.sh \--name 2.6.0-cdh5.7.0 \--tgz \-Phadoop-2.6 \-Phive \-Phive-thriftserver \-Pyarn \-Dhadoop.version=2.6.0-cdh5.7.0 希望这次能没有问题，我们去喝杯咖啡压压惊 这些代码好像写得好多warning~，看着不是很舒服 完成突然眼前飘过一片绿 没错，编译成功了~ 1$ ll 看到这个tgz，至此我们的编译完成！]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.spark基本概念]]></title>
    <url>%2F2018%2F12%2F10%2F6.spark%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[what’s Spark​ Apache Spark™ is a unified analytics engine for large-scale data processing. Spark是用于大规模数据处理的统一分析引擎。 Speed Run workloads 100x faster. Apache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine. ​ Spark是运行在内存上的高速的计算引擎，会比Hadoop快100多倍 Ease of Use123df = spark.read.json("logs.json") df.where("age &gt; 21") .select("name.first").show() ​ 大部分应用都是用Java、Scala、Python、R。Spark还提供了80个高级算子，是我们容易搭建起分布式APP Generality​ 支持Spark SQL,Streaming 和 complex analytics. Run Everywhere You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes. Access data in HDFS, Alluxio, Apache Cassandra, Apache HBase, Apache Hive, and hundreds of other data sources. ​ Spark可以跑在Yarn. Mesos,Kubernetes 独立集群或者云服务。它能访问各种数据源。 Spark生态圈 Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming. 官网这段话很好说明了，spark提供了4种高级工具，Spark SQL、GraphX、MLlib和Spark Streaming， Spark常用术语Application Application：应用。可以认为是多次批量计算组合起来的过程，在物理上可以表现为你写的程序包+部署配置。应用的概念类似于计算机中的程序，它只是一个蓝本，尚没有运行起来。 RDDRDD：Resilient Distributed Datasets，弹性分布式数据集。RDD即是计算模型里的一个概念，也是你编程时用到的一种类。一个RDD可以认为是spark在执行分布式计算时的一批相同来源、相同结构、相同用途的数据集，这个数据集可能被切割成多个分区，分布在不同的机器上，无论如何，这个数据集被称为一个RDD。在编程时，RDD对象就对应了这个数据集，并且RDD对象被当作一个数据操作的基本单位。比如，对某个RDD对象进行map操作，其实就相当于将数据集中的每个分区的每一条数据进行了map映射。 PartitionPartition：分区。一个RDD在物理上被切分为多个Partition，即数据分区，这些Partition可以分布在不同的节点上。Partition是Spark计算任务的基本处理单位，决定了并行计算的粒度，而Partition中的每一条Record为基本处理对象。例如对某个RDD进行map操作，在具体执行时是由多个并行的Task对各自分区的每一条记录进行map映射。 RDD GraphRDD Graph：RDD组成的DAG（有向无环图）。RDD是不可变的，一个RDD经过某种操作后，会生成一个新的RDD。这样说来，一个Application中的程序，其内容基本上都是对各种RDD的操作，从源RDD，经过各种计算，产生中间RDD，最后生成你想要的RDD并输出。这个过程中的各个RDD，会构成一个有向无环图。 LineageLineage：血统。RDD这个概念本身包含了这种信息“由哪个父类RDD经过哪种操作得到”。所以某个RDD可以通过不断寻找父类，找到最原始的那个RDD。这条继承路径就认为是RDD的血统。 Dependency对RDD的Transformation或Action操作，让RDD产生了父子依赖关系（事实上，Transformation或Action操作生成的中间RDD也存在依赖关系），这种依赖分为宽依赖和窄依赖两种： NarrowDependency (窄依赖) parent RDD中的每个Partition最多被child RDD中的一个Partition使用。让RDD产生窄依赖的操作可以称为窄依赖操作，如map、union。 WideDependency (或ShuffleDependency，宽依赖） parent RDD中的每个Partition被child RDD中的多个Partition使用，这时会依据Record的key进行数据重组，这个过程即为Shuffle（洗牌）。让RDD产生宽依赖的操作可以称为宽依赖操作，如reduceByKey, groupByKey。 Spark根据用户Application中的RDD的转换和行动，生成RDD之间的依赖关系，RDD之间的计算链构成了RDD的血统（Lineage），同时也生成了逻辑上的DAG（有向无环图）。每一个RDD都可以根据其依赖关系一级一级向前回溯重新计算，这便是Spark实现容错的一种手段：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 Job在一个Application中，以Action为划分边界的Spark批处理作业。前面提到，Spark采用惰性机制，对RDD的创建和转换并不会立即执行，只有在遇到第一个Action时才会生成一个Job，然后统一调度执行。一个Job包含N个Transformation和1个Action。 Shuffle有一部分Transformation或Action会让RDD产生宽依赖，这样过程就像是将父RDD中所有分区的Record进行了“洗牌”（Shuffle），数据被打散重组，如属于Transformation操作的join，以及属于Action操作的reduce等，都会产生Shuffle。 Stage一个Job中，以Shuffle为边界划分出的不同阶段。每个阶段包含一组可以被串行执行的窄依赖或宽依赖操作： 用户提交的计算任务是一个由RDD构成的DAG，如果RDD在转换的时候需要做Shuffle，那么这个Shuffle的过程就将这个DAG分为了不同的阶段（即Stage）。由于Shuffle的存在，不同的Stage是不能并行计算的，因为后面Stage的计算需要前面Stage的Shuffle的结果。 在对Job中的所有操作划分Stage时，一般会按照倒序进行，即从Action开始，遇到窄依赖操作，则划分到同一个执行阶段，遇到宽依赖操作，则划分一个新的执行阶段，且新的阶段为之前阶段的parent，然后依次类推递归执行。child Stage需要等待所有的parent Stage执行完之后才可以执行，这时Stage之间根据依赖关系构成了一个大粒度的DAG。 在一个Stage内，所有的操作以串行的Pipeline的方式，由一组Task完成计算。 Task对一个Stage之内的RDD进行串行操作的计算任务。每个Stage由一组并发的Task组成（即TaskSet），这些Task的执行逻辑完全相同，只是作用于不同的Partition。一个Stage的总Task的个数由Stage中最后的一个RDD的Partition的个数决定。 Spark Driver会根据数据所在的位置分配计算任务，即把所有Task根据其Partition所在的位置分配给相应的Executor，以尽量减少数据的网络传输（这也就是所谓的移动数据不如移动计算）。一个Executor内同一时刻可以并行执行的Task数由总CPU数／每个Task占用的CPU数决定，即spark.executor.cores / spark.task.cpus。 Task分为ShuffleMapTask和ResultTask两种，位于最后一个Stage的Task为ResultTask，其他阶段的属于ShuffleMapTask。 Persist通过RDD的persist方法，可以将RDD的分区数据持久化在内存或硬盘中，通过cache方法则是缓存到内存。这里的persist和cache是一样的机制，只不过cache是使用默认的MEMORY_ONLY的存储级别对RDD进行persist，故“缓存”也就是一种“持久化”。 前面提到，只有触发了一个Action之后，Spark才会提交Job进行真正的计算。所以RDD只有经过一次Action之后，才能将RDD持久化，然后在Job间共享，即如果两个Job用到了相同的RDD，那么可以在第一个Job中对这个RDD进行缓存，在第二个Job中就避免了RDD的重新计算。持久化机制使需要访问重复数据的Application运行地更快，是能够提升Spark运算速度的一个重要功能。 Checkpoint调用RDD的checkpoint方法，可以将RDD保存到外部存储中，如硬盘或HDFS。Spark引入checkpoint机制，是因为持久化的RDD的数据有可能丢失或被替换，checkpoint可以在这时候发挥作用，避免重新计算。 创建checkpoint是在当前Job完成后，由另外一个专门的Job完成： 也就是说需要checkpoint的RDD会被计算两次。因此，在使用rdd.checkpoint()的时候，建议加上rdd.cache(),这样第二次运行的Job久不用再去计算该rdd了。一个Job在开始处理RDD的Partition时，或者更准确点说，在Executor中运行的任务在获取Partition数据时，会先判断是否被持久化，在没有命中时再判断是否保存了checkpoint，如果没有读取到则会重新计算该Partition。]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala生成本地文件]]></title>
    <url>%2F2018%2F12%2F10%2Fscala%E7%94%9F%E6%88%90%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[题目用Scala文件内容的生成本地的一个文件 格式： url time traffic 例如: http://www.baidu.com [2018-12-08 22:00:00] 30 生成一些错乱数据比如，时间格式不对，流量不为数字 环境IDEA 版本： IntelliJ IDEA 2018.2.5 x64 实现定义一些常量1234567891011121314151617val years = Array("2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019")val urls = Array("https://www.baidu.com/", "https://fenian7788.github.io/", "https://me.csdn.net/weixin_39702831", "https://blog.csdn.net/umdfml00", "https://blog.csdn.net/Jaserok", "https://www.jianshu.com/u/32a08e1b7af0", "https://blog.csdn.net/Sylvia_D507", "https://blog.csdn.net/qq_29416019", "https://blog.csdn.net/qq_39892028", "https://blog.csdn.net/u010854024", "https://blog.csdn.net/qq_34382453", "https://blog.csdn.net/weixin_42733888", )val filePath = "/data/data.txt" 生成随机数1234//生成随机数def generateRandomNum(range: Int) = &#123; (new util.Random).nextInt(range) + 1&#125; 生成有误的随机数1234def generateRandomNum2(range: Int) = &#123; val num = (new util.Random).nextInt(range) if (num % 23 == 0) generateRandomNum(9).toString + "-" else generateRandomNum(100).toString&#125; 生成有误日期1234567891011121314151617181920//会缺大括号的生成年份def generateRandomDate() = &#123; var sb: StringBuilder = new StringBuilder() val randomNum = generateRandomNum(1000) if (randomNum % 97 != 0) sb.append("[") else sb.append(" ") //年份 sb.append(years((new util.Random).nextInt(years.length))).append("-") //月份 sb.append("%02d".format(generateRandomNum(12))).append("-") //日期 sb.append("%02d".format(generateRandomNum(30))).append(" ") //小时 sb.append("%02d".format(generateRandomNum(23))).append(":") //分钟 sb.append("%02d".format(generateRandomNum(59))).append(":") //秒 sb.append("%02d".format(generateRandomNum(59))) if (randomNum % 89 != 0) sb.append("]") else sb.append(" ") sb.toString()&#125; 生成随机url1234//从urls随机获取一个地址def generateRandomUrl(): String = &#123; urls((new util.Random).nextInt(urls.length))&#125; 生成一行123456789def generateLines(): String = &#123; var sb: StringBuilder = new StringBuilder() sb.append(generateRandomUrl()) .append("\t") .append(generateRandomDate()) .append("\t") .append(generateRandomNum2(1000)) sb.toString()&#125; 生成文件12345678def main(args: Array[String]): Unit = &#123; val writer = new PrintWriter(new File(filePath)) for (i &lt;- 1 to 10) &#123; writer.println(generateLines) &#125; writer.flush() writer.close()&#125; 结果 源码GenerateFileApp.scala]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala操作jdbc]]></title>
    <url>%2F2018%2F12%2F10%2Fscala%E6%93%8D%E4%BD%9Cjdbc%2F</url>
    <content type="text"><![CDATA[题目 使用scala操作jdbc (使用 scalikejdbc 来操作) 环境IDEA 版本： IntelliJ IDEA 2018.2.5 x64 实现添加依赖修改pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;github.fenian7788.io&lt;/groupId&gt; &lt;artifactId&gt;scala-training&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;inceptionYear&gt;2008&lt;/inceptionYear&gt; &lt;properties&gt; &lt;scala.version&gt;2.12.5&lt;/scala.version&gt; &lt;scalikejdbc.version&gt;3.3.1&lt;/scalikejdbc.version&gt; &lt;mysql.version&gt;5.1.47&lt;/mysql.version&gt; &lt;logback.version&gt;1.2.3&lt;/logback.version&gt; &lt;/properties&gt; &lt;!-- 添加一个仓库地址 --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-center&lt;/id&gt; &lt;name&gt;Maven Central Repository&lt;/name&gt; &lt;url&gt;http://central.maven.org/maven2/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- scala依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- scalikejdbc依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql jdbc --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 注 ：在阿里云目前好像找不到scalikejdbc的3.3.1版本的 代码初始化12345678import scalikejdbc._val driverClass = "com.mysql.jdbc.Driver"val jdbcUrl = "jdbc:mysql://192.168.137.189:3306/hive";val username = "root";val pwd = "123456";Class.forName(driverClass)ConnectionPool.singleton(jdbcUrl, username, pwd) 建表12345678910def doBefore = &#123; implicit val session = AutoSession sql""" CREATE TABLE IF NOT EXISTS Person( id int PRIMARY KEY NOT NULL auto_increment, name varchar(64), created_time timestamp not null DEFAULT current_timestamp )ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 """.execute.apply()&#125; 插入数据12345//提交一条def doInsert(name: String) = &#123; implicit val session = AutoSession sql"insert into Person(name) values ($&#123;name&#125;)".update().apply()&#125; 更新12345//更新def doUpdate(name: String, id: Int) = &#123; implicit val session = AutoSession sql"update Person set name = $&#123;name&#125; where id = $&#123;id&#125;".update().apply()&#125; 删除12345//删除def doDelete(id: Int) = &#123; implicit val session = AutoSession sql"delete from Person where id = $&#123;id&#125;".execute().apply()&#125; 获取单条记录12345//根据ID获取一条def doGet(id: Int) = &#123; implicit val session = AutoSession sql"select * from person where id = $&#123;id&#125;".map((Person(_))).single().apply().get&#125; 获取列表12345//获取列表def doList() = &#123; implicit val session = AutoSession sql"select * from person".map((Person(_))).list().apply()&#125; 删表1234def doAfter = &#123; implicit val session = AutoSession sql"DROP TABLE IF EXISTS Person".execute.apply()&#125; 关闭连接1ConnectionPool.close() 整体流程123456789101112131415161718192021222324252627282930313233def main(args: Array[String]): Unit = &#123; Class.forName(driverClass) ConnectionPool.singleton(jdbcUrl, username, pwd) //建表 doBefore //提交一条数据 doInsert("Peter") //提交多条数据 doInsert("Mike","Tom","Ken") //查询列表 val list = doList() list.foreach(println) //Person(1,Peter,2018-12-09T15:20:27+08:00[Asia/Shanghai]).... //根据ID获取一条记录 val person1 = doGet(1) println(person1) //Person(1,Peter,2018-12-09T15:20:27+08:00[Asia/Shanghai]) //更新一条记录 val status = doUpdate("Peter Wu",person1.id) println(status) //1 val statusD = doDelete(4) println(statusD) //false --&gt;已经删除了 doAfter ConnectionPool.close()&#125; 我感觉这些API是比较高级的API，等我找到比较底层实现在弄一个 源文件ScalaJdbcApp.scala]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>jdbc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala实现wordCount]]></title>
    <url>%2F2018%2F12%2F10%2Fscala%E5%AE%9E%E7%8E%B0wordCount%2F</url>
    <content type="text"><![CDATA[题目scala实现wordcount 环境IDEA 版本： IntelliJ IDEA 2018.2.5 x64 实现word.txt123hadoop hadoop hadoop word world worldpig hive word hive pig WordCountApp.scala123456789101112131415161718192021222324252627282930313233import scala.io.Source/** * 使用scala实现一个wc小程序 */object WordCountApp &#123; def main(args: Array[String]): Unit = &#123; //文件路径 val filePath = "D:/word.txt" //文件编码 val codec = "utf-8" //打开文件 val file = Source.fromFile(filePath, codec) val wc = file .getLines() // ["hadoop hadoop hadoop","word world world"....] .flatMap(_.split("\t")) //["hadoop","hadoop","hadoop","word"... .toList //List(hadoop, hadoop, hadoop,... .map((_, 1)) //List((hadoop,1), (hadoop,1), (hadoop,1), (word,1),... .groupBy((_._1)) //Map(world -&gt; List((world,1), (world,1)),... .mapValues(_.size) //Map(world -&gt; 2, hadoop -&gt; 3, hive -&gt; 2, word -&gt; 2, pig -&gt; 2) println(wc) // 关闭文件 file.close() &#125; &#125; 结果 源码WordCountApp.scala]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>wordCount</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装LZO]]></title>
    <url>%2F2018%2F12%2F09%2F%E5%AE%89%E8%A3%85LZO%2F</url>
    <content type="text"><![CDATA[环境Linux版本： CentOS 6.5 jdk版本： JDK1.8 hadoop版本： 2.6.0-cdh5.7.0 参考： github上的LZO项目 安装类库安装一些依赖的类库 1# yum -y install lzo-devel zlib-devel gcc autoconf automake libtool 下载、解压LZO123$ cd ~/software$ wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz$ tar -zxvf lzo-2.10.tar.gz -C ../source 编译LZO包123$ cd ~/source/lzo-2.10$./configure --enable-shared --prefix /usr/local/lzo-2.10$ make &amp;&amp; sudo make install 下载Hadoop-LZO123$ cd ~/software$ wget https://github.com/twitter/hadoop-lzo/archive/master.zip$ unzip master.zip -d ../source 修改Hadoop-LZO pom12$ cd ~/source/hadoop-lzo-master$ vi pom.xml 添加cloudera仓库12345678910111213141516&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera-repo&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-nexus-snapshots&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; 修改hadoop版本123456&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!-- &lt;hadoop.current.version&gt;2.6.4&lt;/hadoop.current.version&gt;--&gt; &lt;hadoop.current.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.current.version&gt; &lt;hadoop.old.version&gt;1.0.4&lt;/hadoop.old.version&gt; &lt;/properties&gt; 编译Hadoop-LZO123456789$ C_INCLUDE_PATH=/usr/local/lzo-2.06/include \ LIBRARY_PATH=/usr/local/lzo-2.06/lib \ mvn clean package $ cd target/native/Linux-amd64-64$ tar -cBf - -C lib . | tar -xBvf - -C ~$ mv ~/libgplcompression* $HADOOP_HOME/lib/native/$ cp target/hadoop-lzo-0.4.18-SNAPSHOT.jar \ $HADOOP_HOME/share/hadoop/common/ 配置Hadoop环境变量修改hadoop-env.sh1$ vi $HADOOP_HOME/etc/hadoop/hadoop-env.sh 添加配置 1export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib 修改core-site.xml1$ vi $HADOOP_HOME/etc/hadoop/core-site.xml 添加配置 1234567891011121314&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec, org.apache.hadoop.io.compress.BZip2Codec &lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt; 修改mapred-site.xml123456789101112&lt;property&gt; &lt;name&gt;mapred.compress.map.output&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.child.env&lt;/name&gt; &lt;value&gt;LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib&lt;/value&gt;&lt;/property&gt; 至此我们就完成了LZO的安装了，如果想看如何使用测试LZO的index功能]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>lzo</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试LZO的index功能]]></title>
    <url>%2F2018%2F12%2F09%2F%E6%B5%8B%E8%AF%95LZO%E7%9A%84index%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[题目请测试LZO的index功能​ a) 练习lzo的index如何使用(hadoop-lzo.jar)​ b) block是128M，你的lzo数据&gt;128，请使用一个shell造出来这个数据​ c) 当做wc的input，观察是否是2个map task 环境Linux版本： CentOS 6.5 jdk版本： JDK1.8 hadoop版本： 2.6.0-cdh5.7.0 一份数据：page_views.dat 18.1M linux 有LZO类库：安装LZOM 上传数据123$ cd ~/data## 这个目录上传我们page_views.dat$ rz 放大数据12$ touch create_data.sh$ vi create_data.sh 将我们的数据放大100倍 12345678910#!/bin/bashbasepath=$(cd `dirname $0`; pwd)if [ "$basepath" != "/home/hadoop/data" ];then exitfirm -f page_big900.datfor ((a=1;a&lt;=100;a++))do cat page_views.dat &gt;&gt;page_big900.datdone 运行脚本 1$ sh create_data.sh 压缩数据1$ lzop -9v page_big900.dat 上传到HDFS12$ hdfs dfs -mkdir /user/hadoop/LZO$ hdfs dfs -put page_big900.dat.lzo /user/hadoop/LZO 建立索引文件1$ hadoop jar $HADOOP_HOMOE/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /user/hadoop/LZO/page_big900.dat.lzo 注:也可以使用集群的方式 1$ hadoop jar $HADOOP_HOMOE/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /home/wyp/input/cite.txt.lzo 此时会在hdfs目下生成一个index文件 当做WC的input，运行MR作业当做wc的input，观察是否是以128M为一个split形成一个map task，此次任务按计算应该有 640/128 = 5个 12345$ yarn jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount \ -D mapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat \ -D mapred.output.compress=true \ -D mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec \ /user/hadoop/LZO/page_big900.dat.lzo /user/hadoop/LZO/output 至此，测试完成。]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>lzo</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop从mysql导入到HDFS]]></title>
    <url>%2F2018%2F12%2F07%2Fsqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E5%88%B0HDFS%2F</url>
    <content type="text"><![CDATA[题目sqoop从mysql导入数据到HDFS，请使用snappy压缩​ a) 练习sqoop的用法​ b) 安装snappy并整合使用 环境Linux版本： CentOS 6.5 jdk版本： JDK1.8 hadoop版本： 2.6.0-cdh5.7.0 sqoop版本：1.4.6-cdh5.7.0 hive版本： 1.1.0-cdh5.7.0 mysql版本： 5.6.23 一份数据：page_views.dat 18.1M 检查snappy是否已经安装1$ hadoop checknative 如果你的环境还没有snappy类库请参考编译hadoop源码 创建一张表在mysql 上面创建一张表用来存放我们的数据 1234567891011CREATE TABLE sqoop_export_test( id int PRIMARY KEY NOT NULL auto_increment, event_time datetime, target_url varchar(1000), acc_token varchar(100), aim_url varchar(1000), ip_address varchar(100), comment varchar(100), some_no int) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 注意： 如果数据中存在中文记得加字符集编码 上传数据1234$ cd ~/data## 这个目录上传我们page_views.dat$ rz$ hdfs dfs -put page_views.dat /usr/tmp/ 将数据传到mysql1234567891011$ sqoop export \ --connect jdbc:mysql://192.168.137.189:3306/hive \ --username root \ --password 123456 \ --mapreduce-job-name test1 \ --table sqoop_export_test \ --columns 'event_time,target_url,acc_token,aim_url,ip_address,comment,some_no' \ --export-dir /user/tmp/page_views.dat \ --input-null-string '\\N' \ --input-null-non-string '\\N' \ --input-fields-terminated-by '\t' 将数据从mysql导入hive1234567891011121314$ sqoop import \ --connect jdbc:mysql://192.168.137.189:3306/hive \ --username root \ --password 123456 \ --m 1 \ --mapreduce-job-name aaaa \ --table sqoop_export_test \ --columns 'event_time,target_url,acc_token,aim_url,ip_address,comment,some_no' \ --compression-codec 'snappy' \ --target-dir /user/hadoop/SQOOP_SNAPPY \ --fields-terminated-by ',' \ --delete-target-dir \ --null-non-string '' \ --null-string '' 完成1$ hdfs dfs -ls /user/hadoop/SQOOP_SNAPPY 1$ hdfs dfs -text /user/hadoop/SQOOP_SNAPPY/part-m-00000.snappy 至此，我们完成了sqoop从mysql导入数据到HDFS，以sqoop压缩格式导入。]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>hdfs</tag>
        <tag>snappy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.编译hadoop源码]]></title>
    <url>%2F2018%2F12%2F04%2F5.%E7%BC%96%E8%AF%91hadoop%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[上次我们说到了文件压缩和文件格式，为了本地native包不为空，我们决定自己编译一下Hadoop源码。 环境准备Linux环境：CentOS 6.5 jdk版本：jdk1.7（注：pom文件里面很多注明了1.7，所以最好用1.7编译） maven版本：apache-maven-3.6.0 Hadoop版本：hadoop-2.6.0-cdh5.7.0 依赖jar包库:repo 编译准备编译前我们需要做如下几步： 创建hadoop用户，并在家目录创建app source software repo等目录 部署JAVA环境 安装maven(记得修改本地仓库地址和添加一个阿里maven仓库) 安装Protocol Buffer 2.5.0 前三步我就不写了，我们直接讲如何安装Protocol Buffer 2.5.0 安装Protocol Buffer 2.5.0首先下载Protocol Buffer,这里我已经放到云盘，大家需要自己下载。也可以去官网找。 进到相关目录12# su - hadoop$ cd ~/software 使用rz命令上传1$ rz 解压1$ tar -xzvf protobuf-2.5.0.tar.gz -C ../source 进行安装使用root安装一下类库和命令(如果已经存在可以略过) 123$ exit# yum install -y gcc gcc-c++ make cmake# su - hadoop 回到我们的需要编译的目录 1$ cd ~/source/protobuf-2.5.0 指定安装目录 1$ ./configure --prefix=/home/hadoop/app/protobuf-2.5.0 开始安装 1$ make &amp;&amp; make install 配置用户变量 1$ vi ~/.bash_profile 添加如下语句 12export PROTOC_HOME=/home/hadoop/app/protobuf-2.5.0export PATH=$PROTOC_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$PATH 使环境变量生效 1$ source ~/.bash_profile 验证 1$ protoc --version 安装其它依赖新开一个窗口，使用root安装一些类库 12# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake 编译Hadoop进入相关目录1$ cd ~/software 使用rz命令上传hadoop-2.6.0-cdh5.7.01$ rz 解压1$ tar -zxvf hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ../source 进行安装上传repo库 1234$ cd ~/repo$ rz$ tar -zxvf repo.tar.gz$ mv repo ./ 修改maven的setting.xml，将你的仓库地址指向repo这个目录 1$ vi $MAVEN_HOME/conf/setting.xml 1&lt;localRepository&gt;/home/hadoop/repo&lt;/localRepository&gt; 进入相关目录 12$ cd ~/source/hadoop-2.6.0-cdh5.7.0$ mvn clean package -Pdist,native -DskipTests -Dtar 注意： -DskipTests ：跳过测试-Dtar ：打成tar包 编译成功 注意：如果不使用提供的jar库来编译，过程中会出现很多jar下载失败！ 编译之后我们需要的包在以下的目录，将它拷贝到我们的app下面就可以开始我们hadoop部署了！ 1cp /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0.tar.gz ~/app/]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.初识Hadoop文件格式]]></title>
    <url>%2F2018%2F12%2F03%2F4.%E5%88%9D%E8%AF%86Hadoop%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一般来说，hadoop的存储格式分为行式存储和列式存储 行式存储: SequenceFile,MapFile,Avro Datafile 列式存储: Rcfile,Orcfile,Parquet SequenceFile SequenceFile是Hadoop API 提供的一种二进制文件,它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile,不过它的key为空,使用value 存放实际的值, 这样是为了避免MR 在运行map 阶段的排序过程。 如上图所示，前三个字节是一个Bytes SEQ代表着版本号，同时header也包括key的名称，value class , 压缩细节，metadata，以及Sync markers。Sync markers的作用在于可以读取任意位置的数据。 在record中，又分为是否压缩格式。当没有被压缩时，key与value使用Serialization序列化写入SequenceFile。当选择压缩格式时，record的压缩格式与没有压缩其实不尽相同，除了value的bytes被压缩，key是不被压缩的。 当保存的记录很多时候，可以把一串记录组织到一起同一压缩成一块。 在Block中，它使所有的信息进行压缩，压缩的最小大小由配置文件中，io.seqfile.compress.blocksize配置项决定。 MapFile MapFile是排序后的SequenceFile,通过观察其目录结构可以看到MapFile由两部分组成，分别是data和index。 index作为文件的数据索引，主要记录了每个Record的key值，以及该Record在文件中的偏移位置。在MapFile被访问的时候,索引文件会被加载到内存，通过索引映射关系可迅速定位到指定Record所在文件位置，因此，相对SequenceFile而言，MapFile的检索效率是高效的，缺点是会消耗一部分内存来存储index数据。 需注意的是，MapFile并不会把所有Record都记录到index中去，默认情况下每隔128条记录存储一个索引映射。当然，记录间隔可人为修改，通过MapFIle.Writer的setIndexInterval()方法，或修改io.map.index.interval属性； 另外，与SequenceFile不同的是，MapFile的KeyClass一定要实现WritableComparable接口,即Key值是可比较的。 缺点： 1.文件不支持复写操作，不能向已存在的SequenceFile(MapFile)追加存储记录 2.当write流不关闭的时候，没有办法构造read流。也就是在执行文件写操作的时候，该文件是不可读取的 RcFile RCFile全称Record Columnar File，列式记录文件，是一种类似于SequenceFile的键值对（Key/Value Pairs）数据文件。它的实现原理是首先将表分为几个行组，对每个行组内的数据进行按列存储，每一列的数据都是分开存储，正是先水平划分，再垂直划分的理念。 ​ 如上图是HDFS内RCFile的存储结构，我们可以看到，首先对表进行行划分，分成多个行组。一个行组主要包括：16字节的HDFS同步块信息，主要是为了区分一个HDFS块上的相邻行组；元数据的头部信息主要包括该行组内的存储的行数、列的字段信息等等；数据部分我们可以看出RCFile将每一行，存储为一列，将一列存储为一行，因为当表很大，我们的字段很多的时候，我们往往只需要取出固定的一列就可以。​ 在一般的行存储中 select a from table，虽然只是取出一个字段的值，但是还是会遍历整个表，所以效果和select * from table 一样，在RCFile中，像前面说的情况，只会读取该行组的一行。​ 在一般的列存储中，会将不同的列分开存储，这样在查询的时候会跳过某些列，但是有时候存在一个表的有些列不在同一个HDFS块上，所以在查询的时候，Hive重组列的过程会浪费很多IO开销。而RCFile由于相同的列都是在一个HDFS块上，所以相对列存储而言会节省很多资源。 OrcFileORC是在一定程度上扩展了RCFile，是对RCFile的优化。 ​ 根据结构图，我们可以看到ORCFile在RCFile基础上引申出来Stripe和Footer等。每个ORC文件首先会被横向切分成多个Stripe，而每个Stripe内部以列存储，所有的列存储在一个文件中，而且每个stripe默认的大小是250MB，相对于RCFile默认的行组大小是4MB，所以比RCFile更高效 ​ ORCFile扩展了RCFile的压缩，除了Run-length（游程编码），引入了字典编码和Bit编码。采用字典编码，最后存储的数据便是字典中的值，每个字典值得长度以及字段在字典中的位置 ​ 至于Bit编码，对所有字段都可采用Bit编码来判断该列是否为null，如果为null则Bit值存为0，否则存为1，对于为null的字段在实际编码的时候不需要存储，也就是说字段若为null，是不占用存储空间的。 ParquetParquet是语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件 在Parquet文件中，每一个block都具有一组Row group,她们是由一组Column chunk组成的列数据。继续往下，每一个column chunk中又包含了它具有的pages。每个page就包含了来自于相同列的值.Parquet同时使用更紧凑形式的编码，当写入Parquet文件时，它会自动基于column的类型适配一个合适的编码，比如，一个boolean形式的值将会被用于run-length encoding。 另一方面，Parquet文件对于每个page支持标准的压缩算法比如支持Snappy,gzip以及LZO压缩格式，也支持不压缩。]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.初识Hadoop压缩格式]]></title>
    <url>%2F2018%2F12%2F02%2F3.%E5%88%9D%E8%AF%86Hadoop%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[hadoop文件压缩需求文件压缩对于大数据量的分布式存储系统而言是必须的，它带来有两个好处： 1.减少文件所需的空间 2.加快网络或磁盘间的传输速度 但是也有不好地方： 1.增大了CPU的负载 根据我们的作业性质选择是否需要压缩 压缩分类Compression常见分为两大类 lossless: 无丢失的压缩方式 lossy: 有丢失的压缩方式，常用于视频、图片 下面我们提到的压缩方式都是无丢失的压缩方式 压缩格式​ Hadoop对前面三种有默认集成，有就是说Hadoop支持DEFLATE、Gzip、bzip2三种压缩格式。而后面三种Hadoop没有支持，要用的话要自己去官网下载相应的源码去编译加入到Hadoop才能用 压缩格式 工具 算法 文件扩展名 是否可拆分 gzip gzip default .gz × bzip2 bzip2 bzip2 .bz2 √ LZO LZO LZO .lzo √(YES if indexed) LZ4 LZ4 LZ4 .lz4 × Snappy N/A Snappy .snappy × gzip优点： 压缩率比较高， 解压速度也比较快 hadoop本身支持，linux系统都自带gzip命令 缺点： 不支持split bzip2优点： 压缩比较高 支持split Hadoop本身支持，但不支持native 缺点： 压缩/解压速度比较慢 LZO优点 压缩/解压速度也比较快，合理的压缩率 支持分片，是Hadoop中流行的压缩格式 支持Hadoop native库。 缺点 压缩率比gzip要低一些 Hadoop本身不支持，需要安装 snapp优点: 高速压缩速度和合理的压缩率 支持hadoop native库 缺点 不支持split 压缩率比gzip要低 hadoop本身不支持，需要安装 linux系统下没有对应的命令]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux ssh互信配置]]></title>
    <url>%2F2018%2F11%2F27%2Flinux%20ssh%E4%BA%92%E4%BF%A1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境hadoop001: 192.168.137.190 hadoop002: 192.168.137.191 hadoop003: 192.168.137.192 执行ssh-keygen在3台机子上都执行下面命令1# ssh-keygen 生成一个authorized_keys在第一个hadoop001上面，生成一个authorized_keys文件 12# cd ~/.ssh# cat ~/.ssh/id_rsa.pub&gt;&gt; ~/.ssh/authorized_keys 然后将其他台机子上的id_rsa.pub内容,手动copy到第一台的authorized_keys文件中 先把第二台的公钥拉取过来,需要键入密码 1# scp root@hadoop002:~/.ssh/id_rsa.pub id_rsa.pub002 再把第三台的公钥拉取过来,需要键入密码 1# scp root@hadoop003:~/.ssh/id_rsa.pub id_rsa.pub003 然后将其内容复制到authorized_keys中 12# cat id_rsa.pub002 &gt;&gt; authorized_keys# cat id_rsa.pub003 &gt;&gt; authorized_keys 然后将authorized_keys文件发送给另外两台机子,需要键入密码 12# scp authorized_keys root@hadoop002:~/.ssh/# scp authorized_keys root@hadoop003:~/.ssh/ 然后删除之前拉取的id_rsa.pub002,id_rsa.pub003文件 12# rm -f id_rsa.pub002# rm -f id_rsa.pub003 然后3台机子一起赋权 12# chmod 700 -R ~/.ssh# chmod 600 ~/.ssh/authorized_keys 验证,第一次使用ssh会需要键入yes 123# ssh root@hadoop001 date# ssh root@hadoop002 date# ssh root@hadoop003 date]]></content>
      <categories>
        <category>Vmware安装CentOS</category>
      </categories>
      <tags>
        <tag>vmware</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.搭建Hadoop HA]]></title>
    <url>%2F2018%2F11%2F26%2F2.%E6%90%AD%E5%BB%BAHadoop%20HA%2F</url>
    <content type="text"><![CDATA[准备工作 1.准备好3台虚拟机 2.3台虚拟机互信 3.准备好安装包 jdk-8u161-linux-x64.tar.gz hadoop-2.6.0-cdh5.7.0.tar.gz zookeeper-3.4.12.tar.gz 4.Xshell5 新增用户hadoop在3台虚拟机上面都新建一个hadoop用户 1# useradd hadoop 在3台虚拟机上面都新建目录 12# su - hadoop$ mkdir app source software data tmp 安装JDK创建jdk存放路径12# mkdir -p /usr/java# cd /usr/java 上传jdk压缩包1# rz 解压jdk压缩包12# tar -xzvf jdk-8u161-linux-x64.tar.gz# rm -f jdk-8u161-linux-x64.tar.gz 传输到其他两台12# scp -r jdk1.8.0_161 root@hadoop002:/usr/java# scp -r jdk1.8.0_161 root@hadoop003:/usr/java 修改三台机子上的jdk的持有者1# chown -R root:root /usr/java 配置JDK环境变量1# vi /etc/profile 在文件的末尾加入，以下语句，并保存 123#envexport JAVA_HOME=/usr/java/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATH 使配置生效 1# source /etc/profile 查看jdk是否成功安装 1# java -version 在另外两台机子也一样做一遍，至此我们的jdk就安装完成了！ 集群规划 IP HOST 安装软件 进程 192.168.137.190 hadoop001 Hadoop、Zookeeper NameNode DFSZKFailoverControllerJournalNode DataNode ResourceManager JobHistoryServerNodeManager QuorumPeerMain 192.168.137.191 hadoop002 Hadoop、Zookeeper NameNode DFSZKFailoverControllerJournalNode DataNode ResourceManager NodeManager QuorumPeerMain 192.168.137.192 hadoop003 Hadoop、Zookeeper JournalNode DataNode QuorumPeerMainNodeManager 安装ZK上传压缩包上传ZK的压缩文件文件到hadoop001的software里面 12$ cd software$ rz 传给另外两台机子 12$ scp ~/software/zookeeper-3.4.12.tar.gz hadoop@hadoop002:/home/hadoop/software/$ scp ~/software/zookeeper-3.4.12.tar.gz hadoop@hadoop003:/home/hadoop/software/ 在所有机子上一起运行解压ZK压缩文件到app文件夹里面 12$ cd ~/software$ tar -xzvf zookeeper-3.4.12.tar.gz -C ../app/ 配置ZK_HOME1$ vi ~/.bash_profile 12345678910# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsexport ZK_HOME=/home/hadoop/app/zookeeper-3.4.12export PATH=$ZK_HOME/bin:$PATH 1$ source ~/.bash_profile 修改配置12$ cd ~/app/zookeeper-3.4.12/conf$ cp zoo_sample.cfg zoo.cfg 1$ vi zoo.conf 12345678910111213141516171819202122232425262728293031# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/home/hadoop/data/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to "0" to disable auto purge feature#autopurge.purgeInterval=1server.1=hadoop001:2888:3888server.2=hadoop002:2888:3888server.3=hadoop003:2888:3888 传给其他节点 12$ scp zoo.cfg hadoop@hadoop002:/home/hadoop/app/zookeeper-3.4.12/conf/$ scp zoo.cfg hadoop@hadoop002:/home/hadoop/app/zookeeper-3.4.12/conf/ 在三个节点上 切换到data目录,并创建一个zookeeper目录 12$ cd ~/data$ mkdir zookeeper 在zookeeper目录里面，创建一个myid文件 1$ touch myid 各个节点分开做 然后在hadoop001节点上，在myid文件写入1 1$ echo 1 &gt;myid 在hadoop002节点上，在myid文件写入2 1$ echo 2 &gt;myid 在hadoop003节点上，在myid文件写入3 1$ echo 3 &gt;myid 启动集群1$ zkServer.sh start 安装Hadoop上传hadoop的压缩包在hadoop001机子上使用hadoop用户在software目录上传hadoop的压缩包 12$ cd ~/software$ rz 拷贝到其他两台机子上 12$ scp hadoop-2.6.0-cdh5.7.0.tar.gz hadoop@hadoop002:/home/hadoop/software/$ scp hadoop-2.6.0-cdh5.7.0.tar.gz hadoop@hadoop003:/home/hadoop/software/ 解压缩包三台自己一起执行 12$ cd ~/software$ tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app 配置HADOOP_HOME在hadoop001上面配置一下HADOOP_HOME 1$ vi ~/.bash_profile 1234567891011# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsexport ZK_HOME=/home/hadoop/app/zookeeper-3.4.12export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$PATH 1$ source ~/.bash_profile 修改hadoop-env.sh1$ vi /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh 修改core-site.xml1$ vi /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zzfHadoop&lt;/value&gt; &lt;/property&gt; &lt;!-- ==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不&gt;配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml12$ mkdir -p /home/hadoop/data/dfs/name /home/hadoop/data/dfs/data$ vi /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;!--HDFS超级用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;$&#123;dfs.namenode.name.dir&#125;&lt;/value&gt; &lt;description&gt;namenode存放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为zzfHadoop,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;zzfHadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.zzfHadoop&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.zzfHadoop.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.zzfHadoop.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.zzfHadoop.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.zzfHadoop.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/zzfHadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.zzfHadoop&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改/mapred-site.xml123$ cd ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop$ cp mapred-site.xml.template mapred-site.xml$ vi mapred-site.xml 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt;&lt;!-- 配置 Map段输出的压缩,snappy--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml1$ vi /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 slaves1[hadoop@hadoop001 hadoop]$ vi slaves 123hadoop001hadoop002hadoop003 将配置文件传送到其他节点123$ cd ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/$ scp * hadoop@hadoop002:/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/$ scp * hadoop@hadoop003:/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/ 启动JN集群12$ hadoop-daemon.sh start journalnode$ jps 格式化NameNode在hadoop001节点上输入以下命令 1$ hadoop namenode -format 123456789101112DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.....................18/11/27 17:33:34 INFO namenode.FSImage: Allocated new BlockPoolId: BP-793957562-192.168.137.190-154336881440718/11/27 17:33:34 INFO common.Storage: Storage directory /home/hadoop/data/dfs/name has been successfully formatted.18/11/27 17:33:34 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 018/11/27 17:33:34 INFO util.ExitUtil: Exiting with status 018/11/27 17:33:34 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at hadoop001/192.168.137.190************************************************************/ 同步到第二个NN的目录下 1$ scp -r /home/hadoop/data/dfs/* hadoop@hadoop002:/home/hadoop/data/dfs/ 初始化ZKFC在hadoop001节点上： 1$ hdfs zkfc -formatZK 启动HDFS集群在hadoop001节点上 1$ start-dfs.sh 检查一下 1$ jps| grep -v Jps 启动YARN在hadoop001节点 1$ start-yarn.sh 在hadoop002节点启动RN(standby) 1$ yarn-daemon.sh start resourcemanager 检查一下 1$ jps| grep -v Jps 至此我们集群的搭建就完成了。 关闭集群在hadoop001上 1$ stop-yarn.sh 在hadoop002上 1$ yarn-daemon.sh stop resourcemanager 在hadoop001上 1$ stop-dfs.sh 在三个节点上 1$ zkServer.sh stop 再次启动集群在三个节点上 1$ zkServer.sh start 在hadoop001上 1$ start-dfs.sh 在hadoop001上 1$ start-yarn.sh 在hadoop002上 1$ yarn-daemon.sh start resourcemanager 监控集群在hadoop001上 1$ hdfs dfsadmin -report 开启步骤整理 ZK NN DN JN ZKFC YARN RM(active) NN YARN RM(standby) 关闭步骤整理 YARN RM(active) NN YARN RM(standby) namenode datanode journalnode zkfc ZK]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.Hadoop高级]]></title>
    <url>%2F2018%2F11%2F25%2F1.Hadoop%E9%AB%98%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[回顾Hadoop重点知识​ 我们先来回顾一下Hadoop的一些重要的知识点，虽然在工作中几乎没啥用，但是这些知识点我们需要知道，有助于我们对Hadoop认识。他山之石，可以攻玉，很多知识点都是互相借鉴的。 HDFS 读流程 ​ 1.Client通过Distributed FileSystem模块的open(filePath)向NameNode请求下载文件。 ​ 2.NameNode检查文件是否存在,该用户是否有权限查看，如果没有返回错误信息，否则返回该文件的部分或全部的block列表。（FSDataInputStream对象） ​ 3.Client调用FSdataInputStream对象的read()方法。 ​ 4.就近去Block1的所在的DataNode进行read，读取完后，会校验。假如成功，会关闭与当前DN的通信。假如check fail，会记录失败的快+DN信息，下次就不会再去读取。 ​ 5.然后去最近的DataNode读取Block2的数据，同步骤4一样，如果失败就会去Block2的第二个DN上读取。 ​ 6.去第二个DN获取Block2数据。 ​ 7.Client调用FSDataInputStream.close()，关闭输入流。 整个流程是无感知。 HDFS写流程 ​ 1.Client通过Distributed FileSystem模块的create向NameNode请求写操作。 ​ 2.NameNode会检查该路径下的文件是否已经存在，用户是否用权限去写。假如检查失败，会抛出失败信息，否则就创建一个新文件，但是不关联任何Block，返回一个FSDataOutputStream对象。 ​ 3.Client调用FSDataOutputStream对象的write方法。 ​ 4.FSDataOuputStream对象将第一个块信息写入第一个DataNode，DataNode1写完就传输给DataNode2，DataNode2写完就传输给DataNode3. ​ 5.当DataNode3写完就返回一个 act packet给DataNode2,DataNode2接受DataNode 3的act packet，就发送act packet给DataNode1，当DataNode1 接受到DataNode2 的act packet就会向FSDataOutputStream发送act packet，表示第一个块的 三个副本写完，其他块也依次这样写入。 ​ 6.当全部文件写完，Client调取对象FSDataOutputStream的close方法，关闭输出流。Flush缓存区的数据包。 ​ 7.Client会再次调取Distributed FileSystem的complete方法，告诉NameNode写入成功。 YARN的工作流程 1.用户向Yarn提交应用程序，首先找Resource Manager 分配资源，ResourceManager开启一个container，在container中运行一个Application Manager 2.Application Manager找一台Nodemanager启动Application master，计算任务所需的资源 3.Application Master首先向ResourceManager注册，这样用户就可以通过ResourceManager查询应用程序的运行状态。然后Application将为各个任务申请资源并监控任务的运行状态，知道任务结束。即重复4-7 4.ApplicationMaster采用轮询的方式通过RPC协议向ResourceScheduler申请资源和领取资源 5.一旦ApplicationMaster申请到资源后，便和它对应的NodeManager通信，将资源分配给NodeManager并要求NodeManager启动任务。 6.NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中并通过运行该脚本来启动任务。 7.各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可以随时通过RPC向ApplicationMaster查询应用程序的当前运行状态 应用程序运行完后，ApplicationMaster想ResourceManager注销并关闭自己。 HDFS的HAHDFS HA的角色说明 HDFS HA架构说明 使用2n+1台JN存储editlog，每次写书操作有大多数(大于N+1)返回成功，认为该次写成功，数据不会丢失。（Pasxo算法） 在HA架构里面的Secondary NameNode这个角色已经不存在了，为了使standyby NN实时与Active NN元数据保持一致，他们之间交互通过一系列的轻量级进程JournalNode 任何操作在Active NN上执行的，JN进程同时也会记录editlog到至少半数以上的JN中，这时Standy NN检测到JN里面的同步log发生了变化，会读取JN里面的editlog,然后同步到自己的目录镜像书里面。 当Active NN发生故障的时，Standy NN会在它成为Active NN之前，会同步一次JN里面的editlog，这样就能保持与挂了的Active NN的fsimage一致。然后无缝切换，成为Active NN。达到高可用的目的 DataNode的fencing:确保只有NN能命令DN。 每个NN改变状态的时候，想DN发送自己的状态和一个序列号 DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接受到这个返回则认为该NN为新的Active 如果此时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，此时DN会拒绝这个NN的命令 客户端的fencing：确保只有一个NN能响应客户端请求，让访问Standby NN的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试次数和时间。 ZKFC架构说明 Hadoop提供了ZKFailoverControllor角色，部署在每个NameNode节点上，作为一个Deamon进程，简称ZKFC。它订阅HealthMonitor 和ActiveStandbyElector的事件，并管理NameNode的状态,如下图所示 ZKFailoverController主要包括三个组件： 1.HealthMonitor：监控NameNode是否处于unavailable或者unhealthy状态，当前通过RPC调用NN相应的方法完成。 ActiveStandbyElector：管理和监控自己在ZK中的状态 ZKfailoverController主要职责： 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状体，如果机器宕机，心跳失败，那么zkfc会标记它处于一个不健康状态。 会话管理：如果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态，那么zkfc还会在Zookeeper中华专有一个类型为EPHEMERAL_SEQUENTIAL（临时顺序）的znode，当这个NN挂掉，那么这个znode会被删掉，然后备用的NN的znode会得到这个锁，升级为主NN，同时标记为Active 当宕机的NN重新启动，它会再次zookeeper上注册一个znode，排队等待得到锁，然后自动变成Standby站台，如此往复循环，保证高可靠。 active选举：如上所述，会在zookeeper维持一个znode，来实现抢占式的锁机制，判断哪个NN为Active。 HDFS的Federation 单Active NN的架构使得HDFS在集群扩展和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能瓶颈。 常用的估算公司为1G对应一百万个块，按缺省大小计算的话，大概是64T（这个估算比例是比较大 的富裕的，其实即使每个文件只有一个块，所有元数据信息也不会有1kb/block） 为了解决这个问题，Hadoop 2.x提供了HDFS Federation 多个NN共用一个集群的存储资源，每个NN都可以单独对外提供服务 每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储 DN会按照存储Id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况。 如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但是NN上必须存在相应的目录 设计优势 改动最小，向前兼容，现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话，代码和配置也无需改动 分离命名空间管理和块管理 客户端挂载表：通过路径自动对应NN，是Federation配置改动对应用透明 YARN 的HAYARN HA的角色说明 1.ZKFC:只作为RM进程的一个线程（与HDFS的不一样，HDFS的ZKFC是以一个轻量级守护进程运行的） 2.RMStateStore: a.Rm把Job作业信息存储在ZK里面的/rmstore下，RM Active会向这个目录写App信息 b.当RM(active)挂了，例外的一个RM(standby)成功切换active状态，会从ZK这个/rmstore里面读取相应的作业信息，重新构建作业内存信息，启动内部服务，开始接受NodeManager的心跳，构建集群资源信息，并接受用户的作业提交请求。 3.RM a. 启动的时候会向ZK注册一个Znode并申请一个分布式锁，如果成功，就会成为active，否则就是standby。 b. standby会一直监控这个分布式锁，一旦锁释放(RM active挂了或者运行不良)就会去竞争这个锁，假如成功就成为新的active。 c. 启动和监控ApplicationMaster on NodeManager的container NM 节点资源管理 启动container运行task计算 上报资源 汇报task给ApplicationMaster YARN的HA架构 注意： NM只向RM(active)节点汇报心跳和汇报资源情况(与HDFS不同，HDFS是DN会向NN(active &amp; standby)都汇报)]]></content>
      <categories>
        <category>Big Data learning</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.初识Spark]]></title>
    <url>%2F2018%2F11%2F24%2F1.%E5%88%9D%E8%AF%86Spark%2F</url>
    <content type="text"><![CDATA[what’s Spark​ Apache Spark™ is a unified analytics engine for large-scale data processing. Speed Run workloads 100x faster. Apache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine. ​ Spark是运行在内存上的高速的计算引擎，会比Hadoop快100多倍 Ease of Use123df = spark.read.json("logs.json") df.where("age &gt; 21") .select("name.first").show() ​ 大部分应用都是用Java、Scala、Python、R。Spark还提供了80个高级算子，是我们容易搭建起分布式APP Generality​ 支持Spark SQL,Streaming 和 complex analytics. Run Everywhere You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes. Access data in HDFS, Alluxio, Apache Cassandra, Apache HBase, Apache Hive, and hundreds of other data sources. ​ Spark可以跑在Yarn. Mesos,Kubernetes 独立集群或者云服务。它能访问各种数据源。]]></content>
      <categories>
        <category>Spark预习</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.flink的Word Count小程序]]></title>
    <url>%2F2018%2F11%2F22%2F2.flink%E7%9A%84Word%20Count%E5%B0%8F%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[创建项目打开IDEA，新建一个新的项目，如图所示 填写项目GAV(groupId artifactId Version) 配置你maven设置 设置你的项目地址 创建一个Object 开始编写我们的WordCount1.在pom文件，添加Flink依赖 12345678910111213141516&lt;!-- flink --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2.在刚刚新建的object上面编写，我的object是WordCountApp]]></content>
      <categories>
        <category>Flink系列文章</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vmware安装centOS 6.5(minimal)详细步骤]]></title>
    <url>%2F2018%2F11%2F22%2FVmware%E5%AE%89%E8%A3%85CentOS%206.5%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[​ 我今天整理了一下Vmware安装centOS 6.5详细步骤 ​ Vmware我使用的是14版本的。具体安装步骤就略过了！下面开始我们的CentOS 6.5的系统安装 1.新建虚拟机,点击确定 2.我们使用自定义的，然后会弹出下图，点击下一步 3.选择稍后安装操作系统 4.选择Linux 和版本 CentOS 6 64位 5.输入我们的虚拟机名称和所在的硬盘位置。 6.配置处理器数据量和核数，这个默认就好 7.配置虚拟机的内存，这个根据你的需要，后面如果不够还能改，这里我设置3G 8.设置网络连接方式，默认即可 9.选择IO控制器类型，默认即可 10.选择磁盘类型，默认即可。 11.创建新的虚拟磁盘，点击下一步 12.配置你的虚拟机的硬盘大小，这里我给了40G 13.下一步即可 14.点击完成. 15.如图所示，点击编辑虚拟机设置 16.设置我们的镜像文件 17.开启我们的虚拟机开始安装CentOS 6.5 18.开启虚拟机后，会如下图所示，选择第一个选项即可 19.利用上下左右键选中skip，然后回车，这里是询问是否确认镜像文件是完整的，下载的镜像文件没问题，这里直接跳过即可，进入系统安装界面 20.这里有个警告，我们点击OK即可 21.点击next 22.选择我们语言，这里装个B，选择English 23.选择键盘,我选的是US 24.选择我们的存储设备，第一个是基础存储设备，第二个是指定的。我们选第一个即可 25.这里警告我们设置的磁盘空间可能包含数据(其实没有)，我们选择是的，忽略所有数据 26.配置我们的hostname 27.设置我们的时区 28.设置root用户的密码 29.创建自定义分区 30.create新分区 31.选择标准分区，点击create 32.设置/boot分区为200M 33.设置swap分区2048M 34.设置/根目录获取剩下的磁盘空间 35.分区总览，一般是/boot下面200M，swap分区2G，剩余的都给根目录/ 36.点击next，会弹出格式化警告，我们点击format 37.这个步骤说需要写配置进入我们的硬盘，这个点击write changes to disk 38.点击下一步 39.这里选择最小化安装，然后选择customize now，点击next 40.可以选择一些预安装的服务，然后点击next,如图所示 41.最后重启。 42.我们需要在VMware上面配置一下网络，打开 编辑–&gt;虚拟网络编辑 43.我们之前设置的网络类型是NAT模式，需要选中NAT模式，点击DHCP设置 44.设置我们的虚拟网络的网关，在42步骤的界面上选择NAT设置 45.我们需要配置我们虚拟机的ip 1# vi /etc/sysconfig/network-scripts/ifcfg-eth0 123456789101112DEVICE=eth0HWADDR=00:0C:29:65:28:D9TYPE=EthernetUUID=c206e490-daa4-45a3-9c24-372709e38ee5ONBOOT=yesNM_CONTROLLED=no#BOOTPROTO=dhcpBOOTPROTO="static"IPADDR=192.168.137.190NETMASK=255.255.255.0GATEWAY=192.168.137.2DNS1=192.168.43.1 ​ 其中IPADDR的地址要在我们43步骤设置的范围里面，NETMASK一般是255.255.255.0，GATEWAY是我们设置44步骤，DNS1是我们windows cmd里面打 1ipconfig -all 获取DNS，如图所示 46.关掉防火墙 1#service iptables stop 47.禁止自启动 1#chkconfig iptables off 48.网卡重启 1#service network restart 49.我们可以试试ping 一下www.baidu.com 1#ping www.baidu.com 如图所示，我们的centOS 就部署好了！]]></content>
      <categories>
        <category>Vmware安装CentOS</category>
      </categories>
      <tags>
        <tag>vmware</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.搭建Spring boot]]></title>
    <url>%2F2018%2F11%2F21%2F1.%E6%90%AD%E5%BB%BASpring-boot%2F</url>
    <content type="text"><![CDATA[一、Spring boot简介Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。其实它就是一套集成好的类库，将平时搭建Spring MVC 的依赖都集成在一起，并以一种简单的约定将其运行起来。 Create stand-alone Spring applications Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) Provide opinionated ‘starter’ dependencies to simplify your build configuration Automatically configure Spring and 3rd party libraries whenever possible Provide production-ready features such as metrics, health checks and externalized configuration Absolutely no code generation and no requirement for XML configuration 官网描述有5个特性： 能独立运行的Spring应用 内嵌Tomcat、Jetty或者Undertow (无需WAR包) 提供自用的start依赖去简化你的开发环境搭建 尽可能自动配置Spring和第三方的类库 没有代码生成，也无需XML配置。 二、搭建开发环境下载项目我们访问官网的quick start：https://start.spring.io/ 写好自己项目的GA(Group Artifact),点击Generate project，如图所示 然后，我们会得到一个压缩包。解压后，我们将项目导入IDE，我们的第一步就完成了。 配置依赖在pom.xml文件里面加入 1234567891011121314151617181920212223242526&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 数据库连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- jsp和jSTL依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.flink初识]]></title>
    <url>%2F2018%2F11%2F20%2F1.flink%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[flink是什么？​ flink作为apache的顶级项目之一，我们可以轻松找到它官网的位置http://flink.apache.org。 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. 来自官网的描述：apache flink是基于有界和无界数据流状态的计算的分布式的处理引擎。Flink可以在常见的集群环境，在内存上以任何规模去运行。 Flink基本概念1、处理无界和有界的数据 Any kind of data is produced as a stream of events. Credit card transactions, sensor measurements, machine logs, or user interactions on a website or mobile application, all of these data are generated as a stream. 官网说，所有的数据可以认为是事件流。（举个栗子：信用卡交易数据、传感器测量数据、机器日志、用户在网站或移动应用程序上的交互，这些数据都可以认为是流） 无界数据 Unbounded streams have a start but no defined end. They do not terminate and provide data as it is generated. Unbounded streams must be continuously processed, i.e., events must be promptly handled after they have been ingested. It is not possible to wait for all input data to arrive because the input is unbounded and will not be complete at any point in time. Processing unbounded data often requires that events are ingested in a specific order, such as the order in which events occurred, to be able to reason about result completeness. ​ 其实这个就是说无界数据是源源不断产生的数据,flink提供了对无界数据集的持续计算。需要提供事件顺序才能推断数据结果。 有界数据 Bounded streams have a defined start and end. Bounded streams can be processed by ingesting all data before performing any computations. Ordered ingestion is not required to process bounded streams because a bounded data set can always be sorted. Processing of bounded streams is also known as batch processing. ​ 有界数据集是最终不再发生变化的数据。处理有界数据集不需要事件顺序，因为可以对这个数据进行排序，对有界数据的处理也叫批处理 2、Deploy Applications Anywhere​ Flink它能运行在很多常见资源调度管理器上，例如Hadoop YARN, Apache Mesos和Kubernetes。也能自己独立部署集群。Flink能自行识别所处资源环境，如果发生故障，Flink会通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都通过REST调用进行。 3、Run Applications at any Scale​ Flink能很容易扩展集群规模，可以轻松维护非常大的应用程序状态。 其异步和增量检查点算法确保对处理延迟的影响最小，同时保证一次性状态一致性。 4、Leverage In-Memory Performance​ Flink是高性能的处理引擎，它高效使用内存来计算和存储任务状态，如果可用内存满了，他会存储在访问高效的磁盘上数据结构中，因此它是低延迟的。Flink通过定期和异步检查本地状态到持久存储来保证在出现故障时的一次状态一致性。]]></content>
      <categories>
        <category>Flink系列文章</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的博客]]></title>
    <url>%2F2018%2F11%2F20%2F%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[​ 这里博客仅供参考，主要是个人整理所得。希望大家一起进步]]></content>
  </entry>
</search>
